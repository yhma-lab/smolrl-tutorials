{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A simple example for Reinforcement Learning using table lookup Q-learning method.\n",
    "An agent \"o\" is on the left of a 1 dimensional world, the treasure is on the rightmost location.\n",
    "Run this program and to see how the agent will improve its strategy of finding the treasure.\n",
    "\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "N_states = 6 # 1-dim world\n",
    "Actions = [\"left\", \"right\"]\n",
    "Epsilon = 0.9 # greedy policy\n",
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.9 # discount\n",
    "Max_episodes = 13\n",
    "freshtime = 0.3 # visual\n",
    "\n",
    "def build_q_table(n_states, actions):\n",
    "    q_table = pd.DataFrame(np.zeros((n_states, len(actions))),columns = actions) # action's name\n",
    "    return q_table\n",
    "\n",
    "def choose_actions(state, q_table):\n",
    "    state_actions = q_table.iloc[state,:] # 选取state的整行\n",
    "    if (np.random.uniform() > Epsilon) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value\n",
    "        action_name = np.random.choice(Actions)\n",
    "        # 随机选择or q-table中没有信息，还没有学到\n",
    "    else:\n",
    "        action_name = state_actions.idxmax()\n",
    "    return action_name\n",
    "\n",
    "def get_env_feedback(S, A): # agent\n",
    "    if(A == \"right\"):\n",
    "        if(S == N_states - 2):\n",
    "            S_ = \"terminal\"\n",
    "            R = 1 # 游戏结束，得到reward\n",
    "        else:\n",
    "            S_ = S+1\n",
    "            R = 0\n",
    "    else: # left\n",
    "        R = 0\n",
    "        if(S == 0):\n",
    "            S_ = S\n",
    "        else:\n",
    "            S_ = S-1\n",
    "    return S_, R\n",
    "\n",
    "def update_env(S, episode, step_cnt):\n",
    "    env_list = ['-'] * (N_states-1) + ['T']\n",
    "    if(S == \"terminal\"):\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_cnt)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = \"\".join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(freshtime)\n",
    "    \n",
    "def rl():\n",
    "    q_table = build_q_table(N_states, Actions)\n",
    "    for episode in range(Max_episodes):\n",
    "        step_cnt = 0\n",
    "        S = 0\n",
    "        is_terminal = False\n",
    "        update_env(S, episode, step_cnt)\n",
    "        while(not is_terminal):\n",
    "            A = choose_actions(S, q_table)\n",
    "            S_, R = get_env_feedback(S, A)\n",
    "            q_predict = q_table.loc[S, A]\n",
    "            if(S_ != \"terminal\"):\n",
    "                q_target = R + gamma * q_table.iloc[S_, :].max()\n",
    "            else:\n",
    "                q_target = R\n",
    "                is_terminal = True\n",
    "                \n",
    "            q_table.loc[S, A] += alpha * (q_target - q_predict)\n",
    "            S = S_\n",
    "\n",
    "            update_env(S, episode, step_cnt+1)\n",
    "            step_cnt += 1\n",
    "    return q_table\n",
    "\n",
    "\n",
    "q_table = rl()\n",
    "print(q_table)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
